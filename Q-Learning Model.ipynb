{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from environment import TrafficEnv\n",
    "from ml_controller import Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(Q, epsilon, num_actions):\n",
    "    def policy_fn(observation):\n",
    "        actions = np.ones(num_actions, dtype=float) * epsilon / num_actions\n",
    "        best_action = np.argmax(Q[observation])\n",
    "        actions[best_action] += (1.0 - epsilon)\n",
    "        return actions\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(Q, env, num_episodes, group_size, disc_factor=1, alpha=0.45, epsilon=0.1):\n",
    "    stats = {'episode_lengths': np.zeros(num_episodes)}\n",
    "    \n",
    "    policy = make_epsilon_greedy_policy(Q, epsilon, env.num_actions)\n",
    "    avg_len = 0\n",
    "    \n",
    "    for i_ep  in range(num_episodes):\n",
    "        # Display episode number every so often\n",
    "        if (i_ep + 1) % group_size == 0:\n",
    "            prev_avg = avg_len\n",
    "            avg_len = sum(stats['episode_lengths'][i_ep - group_size - 1:i_ep - 1]) / group_size\n",
    "            avg_diff = avg_len - prev_avg\n",
    "            print(\"\\rEpisode {}/{}, Latest ep len {}, Average change {}\".format(\n",
    "                i_ep + 1, num_episodes, avg_len, avg_diff), end=\"\")\n",
    "        \n",
    "        prev_state = tuple(env.reset())\n",
    "        \n",
    "        for t in itertools.count():\n",
    "            action_probs = policy(prev_state)\n",
    "            action = np.random.choice(np.arange(env.num_actions), p=action_probs)\n",
    "            reward, done = env.step(action)\n",
    "            next_state = tuple(env.state)\n",
    "            \n",
    "            stats['episode_lengths'][i_ep] = t\n",
    "            \n",
    "            best_next_action = np.argmax(Q[next_state])\n",
    "            td_target = reward + disc_factor * Q[next_state][best_next_action]\n",
    "            td_delta = td_target - Q[prev_state][action]\n",
    "            Q[prev_state][action] += alpha * td_delta\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            prev_state = next_state\n",
    "    return Q, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dd():\n",
    "    return np.zeros(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cars = 5\n",
    "episode_length = 20000\n",
    "group_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TrafficEnv()\n",
    "controller = Controller(env, max_cars)\n",
    "\n",
    "# Q maps the state -> (action -> action-value)\n",
    "Q = defaultdict(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 19000/20000, Latest ep len 177.898, Average change 27.264999999999986"
     ]
    }
   ],
   "source": [
    "Q, stats = q_learning(Q, controller, episode_length, group_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_len = stats['episode_lengths']\n",
    "avg_ep_len_arr = []\n",
    "for ep_range in range(0, episode_length, group_size):\n",
    "    avg_ep_len = sum(ep_len[ep_range:ep_range+group_size]) / group_size\n",
    "    avg_ep_len_arr.append(avg_ep_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(list(range(len(avg_ep_len_arr))), avg_ep_len_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(Q, open('policy.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_env = TrafficEnv()\n",
    "sim = Controller(sim_env, max_cars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_cars = 0\n",
    "sim.reset()\n",
    "for _ in range(100):\n",
    "    best_action = np.argmax(Q[sim.state])\n",
    "    print(f'The lane state is {sim.lane_state}')\n",
    "    print(f'The state is {sim.state}')\n",
    "    rewards, done  = sim.step(best_action)\n",
    "    total_cars += sum(sim.lane_state)\n",
    "    print(f'The action returns are {list(Q[sim.state])}')\n",
    "    print(f'The action taken was {best_action}')\n",
    "    print()\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print('The model did not last for the full simulation')\n",
    "print(f'The model survived for {_} seconds')\n",
    "avg_cars = total_cars/_\n",
    "print(f'Average cars during runtime: {avg_cars}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
